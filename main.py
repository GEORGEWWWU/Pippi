import requests
from bs4 import BeautifulSoup
import re
import os
import time
import random
import hashlib
from urllib.parse import urlparse, unquote
from pathlib import Path


class RobustImageSpider:
    def __init__(self, download_folder="pippi_images"):
        self.download_folder = Path(download_folder)
        self.session = requests.Session()

        # å¤šä¸ª User-Agent è½®æ¢
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
        ]

        # åŸºç¡€ headersï¼Œ**ç»ä¸åŒ…å« Referer**
        self.session.headers.update({
            'User-Agent': self.user_agents[0],
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9',
        })

        self.download_folder.mkdir(parents=True, exist_ok=True)
        self.downloaded_count = 0
        self.skipped_count = 0
        self.failed_count = 0

        # æ”¯æŒå¤šç§å›¾ç‰‡æ ¼å¼
        self.image_extensions = ('.jpg', '.jpeg', '.png', '.webp', '.gif', '.bmp', '.tiff')

        # åŠ è½½å·²ä¸‹è½½çš„æ–‡ä»¶åˆ—è¡¨
        self.existing_files = self._load_existing_files()

    def _load_existing_files(self):
        """åŠ è½½å·²å­˜åœ¨çš„æ–‡ä»¶åˆ—è¡¨"""
        existing = set()
        if self.download_folder.exists():
            for f in self.download_folder.iterdir():
                if f.is_file():
                    existing.add(f.stem)
        print(f"ğŸ“‚ å‘ç° {len(existing)} ä¸ªå·²ä¸‹è½½çš„æ–‡ä»¶ï¼Œå°†è‡ªåŠ¨è·³è¿‡")
        return existing

    def _get_random_delay(self, min_sec=1.5, max_sec=3.5):
        """éšæœºå»¶è¿Ÿ"""
        return random.uniform(min_sec, max_sec)

    def get_page(self, url, retries=3):
        """è·å–é¡µé¢ï¼Œå¸¦é‡è¯•"""
        for attempt in range(retries):
            try:
                time.sleep(self._get_random_delay(0.5, 1.5))

                headers = {
                    'User-Agent': random.choice(self.user_agents),
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                }

                r = self.session.get(url, headers=headers, timeout=15)
                r.raise_for_status()
                return r.text
            except Exception as e:
                print(f"  âš ï¸ è·å–å¤±è´¥ (å°è¯• {attempt + 1}/{retries}): {str(e)[:50]}")
                if attempt < retries - 1:
                    time.sleep(2 ** attempt)
        return None

    def extract_images(self, html, base_url=None):
        """æ™ºèƒ½æå–å›¾ç‰‡é“¾æ¥ - æ”¯æŒå¤šç§æ¨¡å¼"""
        if not html:
            return []

        images = []

        # æ–¹æ³•1: é€šç”¨æ­£åˆ™ - åŒ¹é…æ‰€æœ‰å¸¸è§å›¾ç‰‡æ ¼å¼
        # åŒ¹é…ä»»æ„æ–‡ä»¶åï¼Œä¸é™äºDSCå‰ç¼€
        generic_pattern = r'https?://[^\s"<>\']+?\.(?:jpg|jpeg|png|webp|gif|bmp|tiff)(?:\?[^\s"<>\']*)?'
        matches = re.findall(generic_pattern, html, re.IGNORECASE)
        images.extend(matches)

        # æ–¹æ³•2: é’ˆå¯¹ç‰¹å®šCDNä¼˜åŒ–ï¼ˆå¦‚acg.lolï¼‰
        cdn_pattern = r'https?://image\.acg\.lol/file/\d{4}/\d{2}/\d{2}/[^"\s<>\']+\.(?:jpg|jpeg|png|webp)'
        cdn_matches = re.findall(cdn_pattern, html, re.IGNORECASE)
        images.extend(cdn_matches)

        # æ–¹æ³•3: BeautifulSoupè§£æï¼ˆå¤‡ç”¨ï¼‰
        if len(images) < 5:  # å¦‚æœæ­£åˆ™åŒ¹é…å¤ªå°‘ï¼Œå¯ç”¨BS4
            soup = BeautifulSoup(html, 'html.parser')
            for img in soup.find_all('img'):
                for attr in ['src', 'data-src', 'data-original', 'data-url']:
                    src = img.get(attr)
                    if src:
                        # å¤„ç†ç›¸å¯¹è·¯å¾„
                        if src.startswith('//'):
                            src = 'https:' + src
                        elif src.startswith('/'):
                            if base_url:
                                src = base_url.rstrip('/') + src
                        elif not src.startswith('http'):
                            continue

                        # æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡
                        if any(src.lower().endswith(ext) for ext in self.image_extensions) or \
                                any(ext in src.lower() for ext in self.image_extensions):
                            images.append(src)
                        break

        # æ–¹æ³•4: åŒ¹é…æ‡’åŠ è½½/èƒŒæ™¯å›¾ç­‰
        lazy_pattern = r'(?:data-|original-|lazy-)(?:src|url)["\']?\s*[=:]\s*["\']?(https?://[^\s"<>\']+?\.(?:jpg|jpeg|png|webp))'
        lazy_matches = re.findall(lazy_pattern, html, re.IGNORECASE)
        images.extend(lazy_matches)

        # æ¸…ç†å’Œå»é‡
        cleaned = []
        seen = set()
        for url in images:
            # æ¸…ç†URL
            url = url.strip().rstrip('"\'').replace('\\/', '/')

            # å»é‡
            if url and url not in seen:
                seen.add(url)
                cleaned.append(url)

        print(f"ğŸ” æ‰¾åˆ° {len(cleaned)} ä¸ªå›¾ç‰‡é“¾æ¥")
        if cleaned:
            print(f"   ç¤ºä¾‹: {cleaned[0][:60]}...")
        return cleaned

    def _get_filename(self, url, index):
        """æ™ºèƒ½ç”Ÿæˆæ–‡ä»¶å - ä¿ç•™åŸå§‹æ–‡ä»¶åæˆ–ç”Ÿæˆæ–°åç§°"""
        try:
            # URLè§£ç 
            decoded_url = unquote(url)
            parsed = urlparse(decoded_url)
            path = parsed.path

            # å°è¯•æå–åŸå§‹æ–‡ä»¶å
            original_name = Path(path).name

            # æ¸…ç†æ–‡ä»¶å
            if original_name and '.' in original_name:
                # ç§»é™¤æŸ¥è¯¢å‚æ•°å’Œéæ³•å­—ç¬¦
                clean_name = re.sub(r'[<>:"/\\|?*]', '_', original_name)
                clean_name = clean_name.split('?')[0]  # ç§»é™¤URLå‚æ•°

                # é™åˆ¶é•¿åº¦
                name = Path(clean_name).stem[:50]  # é™åˆ¶50å­—ç¬¦
                ext = Path(clean_name).suffix.lower()

                # ç¡®ä¿æ‰©å±•ååˆæ³•
                if ext not in self.image_extensions:
                    ext = '.jpg'

                return name, ext

        except Exception:
            pass

        # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ç´¢å¼•+å“ˆå¸Œ
        url_hash = hashlib.md5(url.encode()).hexdigest()[:6]
        return f"img_{index:04d}_{url_hash}", ".jpg"

    def _is_exists(self, filename_stem):
        """æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨"""
        if filename_stem in self.existing_files:
            return True
        for ext in self.image_extensions:
            if (self.download_folder / f"{filename_stem}{ext}").exists():
                return True
        return False

    def download_image(self, url, index, retries=3):
        """ä¸‹è½½å•å¼ å›¾ç‰‡"""
        filename_stem, ext = self._get_filename(url, index)

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if self._is_exists(filename_stem):
            self.skipped_count += 1
            print(f"  â­ï¸  [{index}] {filename_stem}{ext} (å·²å­˜åœ¨)")
            return True

        for attempt in range(retries):
            try:
                delay = min(1.5 + self.downloaded_count * 0.03, 5)
                time.sleep(random.uniform(delay, delay + 1.5))

                headers = {
                    'User-Agent': random.choice(self.user_agents),
                    'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
                }

                r = self.session.get(url, headers=headers, timeout=20, stream=True)
                r.raise_for_status()

                filepath = self.download_folder / f"{filename_stem}{ext}"
                total_size = 0

                with open(filepath, 'wb') as f:
                    for chunk in r.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            total_size += len(chunk)

                if total_size < 1024:
                    filepath.unlink()
                    raise ValueError("æ–‡ä»¶è¿‡å°")

                self.existing_files.add(filename_stem)
                self.downloaded_count += 1

                size_kb = total_size / 1024
                print(f"  âœ“ [{index}] {filename_stem}{ext} ({size_kb:.1f} KB)")
                return True

            except Exception as e:
                if attempt < retries - 1:
                    time.sleep(2 ** attempt + random.uniform(0, 1))
                else:
                    self.failed_count += 1
                    print(f"  âŒ [{index}] å¤±è´¥: {str(e)[:40]}")

        return False

    def crawl(self, target_url):
        """ä¸»å‡½æ•°"""
        print(f"\n{'=' * 60}")
        print(f"ğŸš€ çˆ¬å–: {target_url}")
        print(f"ğŸ“ ç›®å½•: {self.download_folder.absolute()}")
        print(f"{'=' * 60}\n")

        html = self.get_page(target_url)
        if not html:
            print("âŒ è·å–é¡µé¢å¤±è´¥")
            return 0

        images = self.extract_images(html, base_url=target_url)
        total = len(images)

        if not images:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡")
            return 0

        print(f"ğŸ¯ å…± {total} å¼ å›¾ç‰‡ï¼Œå¼€å§‹ä¸‹è½½...\n")

        for i, url in enumerate(images, 1):
            self.download_image(url, i)

            if i % 10 == 0 and i < total:
                rest = random.uniform(3, 6)
                print(f"ğŸ’¤ ä¼‘æ¯ {rest:.1f} ç§’...")
                time.sleep(rest)

        print(f"\n{'=' * 60}")
        print(f"âœ… å®Œæˆ: æ–°ä¸‹è½½ {self.downloaded_count}, è·³è¿‡ {self.skipped_count}, å¤±è´¥ {self.failed_count}")
        print(f"{'=' * 60}")

        return self.downloaded_count


def get_user_input():
    """è·å–ç”¨æˆ·è¾“å…¥çš„URL"""
    print("\n" + "=" * 60)
    print("ğŸ•·ï¸  æ¬¢è¿ä½¿ç”¨çš®çš®è››å›¾ç‰‡ä¸‹è½½å™¨")
    print("   æ”¯æŒä»»æ„æ ¼å¼: JPG PNG WEBP GIF ç­‰")
    print("=" * 60)

    default_url = "https://bing.fullpx.com/"
    print(f"\nğŸ’¡ æç¤ºï¼šç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤é“¾æ¥")
    print(f"   é»˜è®¤: {default_url}")

    while True:
        try:
            user_input = input("\nğŸ”— è¯·è¾“å…¥è¦çˆ¬å–çš„é¡µé¢é“¾æ¥: ").strip()

            if not user_input:
                print(f"âœ“ ä½¿ç”¨é»˜è®¤é“¾æ¥")
                return default_url

            if not user_input.startswith(('http://', 'https://')):
                print("âš ï¸  é“¾æ¥å¿…é¡»ä»¥ http:// æˆ– https:// å¼€å¤´")
                continue

            if '.' not in user_input:
                print("âš ï¸  é“¾æ¥æ ¼å¼ä¸æ­£ç¡®")
                continue

            return user_input

        except KeyboardInterrupt:
            return None


def get_folder_name():
    """è·å–ä¿å­˜æ–‡ä»¶å¤¹åç§°"""
    default_folder = "pippi_images"
    print(f"\nğŸ’¡ æç¤ºï¼šç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤æ–‡ä»¶å¤¹ '{default_folder}'")

    try:
        folder = input("ğŸ“ è¯·è¾“å…¥ä¿å­˜æ–‡ä»¶å¤¹åç§°: ").strip()
        if not folder:
            return default_folder

        folder = re.sub(r'[<>:"/\\|?*]', '_', folder)
        return folder or default_folder

    except KeyboardInterrupt:
        return default_folder


def confirm_download(url, folder):
    """ç¡®è®¤ä¸‹è½½ä¿¡æ¯"""
    print("\n" + "-" * 60)
    print("ğŸ“‹ ä¸‹è½½ä¿¡æ¯ç¡®è®¤:")
    print(f"   ç›®æ ‡é“¾æ¥: {url}")
    print(f"   ä¿å­˜ç›®å½•: {Path(folder).absolute()}")
    print("-" * 60)

    try:
        confirm = input("ğŸš€ ç¡®è®¤å¼€å§‹ä¸‹è½½? [Y/n]: ").strip().lower()
        return confirm in ('', 'y', 'yes', 'æ˜¯')
    except KeyboardInterrupt:
        return False


def main():
    """ä¸»å‡½æ•°"""
    try:
        target_url = get_user_input()
        if not target_url:
            print("âŒ æœªæä¾›æœ‰æ•ˆé“¾æ¥")
            return

        folder_name = get_folder_name()

        if not confirm_download(target_url, folder_name):
            print("âŒ ç”¨æˆ·å–æ¶ˆä¸‹è½½")
            return

        spider = RobustImageSpider(folder_name)
        spider.crawl(target_url)

        # å¾ªç¯ä¸‹è½½
        while True:
            try:
                print("\n" + "=" * 60)
                again = input("ğŸ”„ æ˜¯å¦ç»§ç»­ä¸‹è½½å…¶ä»–é“¾æ¥? [y/N]: ").strip().lower()
                if again not in ('y', 'yes', 'æ˜¯'):
                    print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                    break

                new_url = get_user_input()
                if not new_url:
                    break

                new_folder = get_folder_name()
                if not confirm_download(new_url, new_folder):
                    continue

                spider = RobustImageSpider(new_folder)
                spider.crawl(new_url)

            except KeyboardInterrupt:
                break

    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå‡ºé”™: {e}")
    finally:
        input("\næŒ‰å›è½¦é”®é€€å‡º...")


if __name__ == "__main__":
    main()

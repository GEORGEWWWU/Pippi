import requests
from bs4 import BeautifulSoup
import re
import os
import time
import random
import hashlib
from urllib.parse import urlparse
from pathlib import Path


class RobustImageSpider:
    def __init__(self, download_folder="pippi_images"):
        self.download_folder = Path(download_folder)
        self.session = requests.Session()

        # å¤šä¸ª User-Agent è½®æ¢
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
        ]

        # åŸºç¡€ headersï¼Œ**ç»ä¸åŒ…å« Referer**
        self.session.headers.update({
            'User-Agent': self.user_agents[0],
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9',
        })

        self.download_folder.mkdir(parents=True, exist_ok=True)
        self.downloaded_count = 0
        self.skipped_count = 0
        self.failed_count = 0

        # åŠ è½½å·²ä¸‹è½½çš„æ–‡ä»¶åˆ—è¡¨
        self.existing_files = self._load_existing_files()

    def _load_existing_files(self):
        """åŠ è½½å·²å­˜åœ¨çš„æ–‡ä»¶åˆ—è¡¨"""
        existing = set()
        if self.download_folder.exists():
            for f in self.download_folder.iterdir():
                if f.is_file():
                    existing.add(f.stem)
        print(f"ğŸ“‚ å‘ç° {len(existing)} ä¸ªå·²ä¸‹è½½çš„æ–‡ä»¶ï¼Œå°†è‡ªåŠ¨è·³è¿‡")
        return existing

    def _get_random_delay(self, min_sec=1.5, max_sec=3.5):
        """éšæœºå»¶è¿Ÿ"""
        return random.uniform(min_sec, max_sec)

    def get_page(self, url, retries=3):
        """è·å–é¡µé¢ï¼Œå¸¦é‡è¯•"""
        for attempt in range(retries):
            try:
                time.sleep(self._get_random_delay(0.5, 1.5))

                # éšæœºåˆ‡æ¢ User-Agent
                headers = {
                    'User-Agent': random.choice(self.user_agents),
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                }
                # **æ³¨æ„ï¼šè¿™é‡Œæ²¡æœ‰ Refererï¼**

                r = self.session.get(url, headers=headers, timeout=15)
                r.raise_for_status()
                return r.text
            except Exception as e:
                print(f"  âš ï¸ è·å–å¤±è´¥ (å°è¯• {attempt + 1}/{retries}): {str(e)[:50]}")
                if attempt < retries - 1:
                    time.sleep(2 ** attempt)
        return None

    def extract_images(self, html):
        """æå–å›¾ç‰‡é“¾æ¥ - ä¿æŒåŸæ¥çš„ç®€å•ç²—æš´æ–¹å¼"""
        if not html:
            return []

        # **å…³é”®ï¼šä¿æŒåŸæ¥çš„æ­£åˆ™ï¼Œä¸€æ¨¡ä¸€æ ·ï¼**
        pattern = r'https?://image\.acg\.lol/file/\d{4}/\d{2}/\d{2}/DSC\d+\.jpg'
        matches = re.findall(pattern, html)

        # å»é‡ä½†ä¿æŒé¡ºåº
        seen = set()
        unique = []
        for url in matches:
            if url not in seen:
                seen.add(url)
                unique.append(url)

        print(f"ğŸ” è°ƒè¯•ï¼šæ‰¾åˆ° {len(unique)} ä¸ªåŒ¹é…")  # è°ƒè¯•ç”¨ï¼Œå¯ä»¥çœ‹åˆ°æ˜¯å¦åŒ¹é…åˆ°
        return unique

    def _get_filename(self, url, index):
        """ç”Ÿæˆæ–‡ä»¶å"""
        # å°è¯•æå– DSC ç¼–å·
        match = re.search(r'DSC(\d+)\.jpg', url)
        if match:
            return f"DSC{match.group(1)}", ".jpg"

        # å¤‡ç”¨æ–¹æ¡ˆ
        url_hash = hashlib.md5(url.encode()).hexdigest()[:6]
        return f"img_{index:04d}_{url_hash}", ".jpg"

    def _is_exists(self, filename_stem):
        """æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨"""
        if filename_stem in self.existing_files:
            return True
        # å†æ£€æŸ¥æ–‡ä»¶ç³»ç»Ÿ
        for ext in ['.jpg', '.jpeg', '.png']:
            if (self.download_folder / f"{filename_stem}{ext}").exists():
                return True
        return False

    def download_image(self, url, index, retries=3):
        """ä¸‹è½½å•å¼ å›¾ç‰‡"""
        filename_stem, ext = self._get_filename(url, index)

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if self._is_exists(filename_stem):
            self.skipped_count += 1
            print(f"  â­ï¸  [{index}] {filename_stem}{ext} (å·²å­˜åœ¨)")
            return True

        for attempt in range(retries):
            try:
                # é€’å¢å»¶è¿Ÿ
                delay = min(1.5 + self.downloaded_count * 0.03, 5)
                time.sleep(random.uniform(delay, delay + 1.5))

                # **å…³é”®ï¼šä¸‹è½½å›¾ç‰‡æ—¶ä¸å¸¦ Refererï¼**
                headers = {
                    'User-Agent': random.choice(self.user_agents),
                    'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
                    # æ²¡æœ‰ Refererï¼
                }

                r = self.session.get(url, headers=headers, timeout=20, stream=True)
                r.raise_for_status()

                filepath = self.download_folder / f"{filename_stem}{ext}"
                total_size = 0

                with open(filepath, 'wb') as f:
                    for chunk in r.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            total_size += len(chunk)

                # éªŒè¯æ–‡ä»¶
                if total_size < 1024:
                    filepath.unlink()
                    raise ValueError("æ–‡ä»¶è¿‡å°")

                self.existing_files.add(filename_stem)
                self.downloaded_count += 1

                size_kb = total_size / 1024
                print(f"  âœ“ [{index}] {filename_stem}{ext} ({size_kb:.1f} KB)")
                return True

            except Exception as e:
                if attempt < retries - 1:
                    time.sleep(2 ** attempt + random.uniform(0, 1))
                else:
                    self.failed_count += 1
                    print(f"  âŒ [{index}] å¤±è´¥: {str(e)[:40]}")

        return False

    def crawl(self, target_url):
        """ä¸»å‡½æ•°"""
        print(f"\n{'=' * 60}")
        print(f"ğŸš€ çˆ¬å–: {target_url}")
        print(f"ğŸ“ ç›®å½•: {self.download_folder.absolute()}")
        print(f"{'=' * 60}\n")

        html = self.get_page(target_url)
        if not html:
            print("âŒ è·å–é¡µé¢å¤±è´¥")
            return 0

        # è°ƒè¯•ç”¨ï¼šä¿å­˜ HTML çœ‹çœ‹å†…å®¹
        # with open('debug.html', 'w', encoding='utf-8') as f:
        #     f.write(html[:5000])

        images = self.extract_images(html)
        total = len(images)

        if not images:
            print("âš ï¸ æœªæ‰¾åˆ°å›¾ç‰‡ï¼Œå°è¯•å¤‡ç”¨æå–æ–¹æ³•...")
            # å¤‡ç”¨ï¼šç”¨ BeautifulSoup æ‰¾æ‰€æœ‰å›¾ç‰‡
            soup = BeautifulSoup(html, 'html.parser')
            for img in soup.find_all('img'):
                src = img.get('src', '')
                if 'acg.lol' in src:
                    images.append(src)
            images = list(dict.fromkeys(images))  # å»é‡
            total = len(images)
            print(f"ğŸ” å¤‡ç”¨æ–¹æ³•æ‰¾åˆ° {total} ä¸ª")

        if not images:
            print("âŒ ç¡®å®æ²¡æœ‰å›¾ç‰‡")
            return 0

        print(f"ğŸ¯ å…± {total} å¼ å›¾ç‰‡ï¼Œå¼€å§‹ä¸‹è½½...\n")

        for i, url in enumerate(images, 1):
            self.download_image(url, i)

            # æ¯10å¼ ä¼‘æ¯ä¸€ä¸‹
            if i % 10 == 0 and i < total:
                rest = random.uniform(3, 6)
                print(f"ğŸ’¤ ä¼‘æ¯ {rest:.1f} ç§’...")
                time.sleep(rest)

        print(f"\n{'=' * 60}")
        print(f"âœ… å®Œæˆ: æ–°ä¸‹è½½ {self.downloaded_count}, è·³è¿‡ {self.skipped_count}, å¤±è´¥ {self.failed_count}")
        print(f"{'=' * 60}")

        return self.downloaded_count


def get_user_input():
    """è·å–ç”¨æˆ·è¾“å…¥çš„URLï¼Œæ”¯æŒéªŒè¯å’Œé»˜è®¤ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("ğŸ•·ï¸  æ¬¢è¿ä½¿ç”¨çš®çš®è››å›¾ç‰‡ä¸‹è½½å™¨")
    print("=" * 60)

    # æ˜¾ç¤ºé»˜è®¤ç¤ºä¾‹
    default_url = "https://bing.fullpx.com/"
    print(f"\nğŸ’¡ æç¤ºï¼šç›´æ¥å›è½¦å°†ä½¿ç”¨é»˜è®¤é“¾æ¥")
    print(f"   é»˜è®¤: {default_url}")

    while True:
        try:
            user_input = input("\nğŸ”— è¯·è¾“å…¥è¦çˆ¬å–çš„é¡µé¢é“¾æ¥: ").strip()

            # å¦‚æœç”¨æˆ·ç›´æ¥å›è½¦ï¼Œä½¿ç”¨é»˜è®¤é“¾æ¥
            if not user_input:
                print(f"âœ“ ä½¿ç”¨é»˜è®¤é“¾æ¥")
                return default_url

            # åŸºç¡€URLéªŒè¯
            if not user_input.startswith(('http://', 'https://')):
                print("âš ï¸  é“¾æ¥å¿…é¡»ä»¥ http:// æˆ– https:// å¼€å¤´")
                continue

            # ç®€å•éªŒè¯URLæ ¼å¼
            if '.' not in user_input:
                print("âš ï¸  é“¾æ¥æ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·æ£€æŸ¥")
                continue

            print(f"âœ“ å·²è¾“å…¥é“¾æ¥: {user_input}")
            return user_input

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            return None
        except Exception as e:
            print(f"âš ï¸  è¾“å…¥é”™è¯¯: {e}")


def get_folder_name():
    """è·å–ä¿å­˜æ–‡ä»¶å¤¹åç§°"""
    default_folder = "pippi_images"
    print(f"\nğŸ’¡ æç¤ºï¼šç›´æ¥å›è½¦å°†ä½¿ç”¨é»˜è®¤æ–‡ä»¶å¤¹ '{default_folder}'")

    try:
        folder = input("ğŸ“ è¯·è¾“å…¥ä¿å­˜æ–‡ä»¶å¤¹åç§°: ").strip()
        if not folder:
            print(f"âœ“ ä½¿ç”¨é»˜è®¤æ–‡ä»¶å¤¹: {default_folder}")
            return default_folder

        # æ¸…ç†éæ³•å­—ç¬¦
        folder = re.sub(r'[<>:"/\\|?*]', '_', folder)
        if not folder:
            folder = "downloaded_images"

        print(f"âœ“ ä¿å­˜è‡³æ–‡ä»¶å¤¹: {folder}")
        return folder

    except KeyboardInterrupt:
        print(f"\nâœ“ ä½¿ç”¨é»˜è®¤æ–‡ä»¶å¤¹: {default_folder}")
        return default_folder


def confirm_download(url, folder):
    """ç¡®è®¤ä¸‹è½½ä¿¡æ¯"""
    print("\n" + "-" * 60)
    print("ğŸ“‹ ä¸‹è½½ä¿¡æ¯ç¡®è®¤:")
    print(f"   ç›®æ ‡é“¾æ¥: {url}")
    print(f"   ä¿å­˜ç›®å½•: {Path(folder).absolute()}")
    print("-" * 60)

    try:
        confirm = input("ğŸš€ ç¡®è®¤å¼€å§‹ä¸‹è½½? [Y/n]: ").strip().lower()
        return confirm in ('', 'y', 'yes', 'æ˜¯', 'ç¡®è®¤')
    except KeyboardInterrupt:
        return False


def main():
    """ä¸»å‡½æ•°ï¼šäº¤äº’å¼å…¥å£"""
    try:
        # è·å–ç”¨æˆ·è¾“å…¥
        target_url = get_user_input()
        if not target_url:
            print("âŒ æœªæä¾›æœ‰æ•ˆé“¾æ¥ï¼Œç¨‹åºé€€å‡º")
            return

        folder_name = get_folder_name()

        # ç¡®è®¤ä¸‹è½½
        if not confirm_download(target_url, folder_name):
            print("âŒ ç”¨æˆ·å–æ¶ˆä¸‹è½½")
            return

        # åˆ›å»ºçˆ¬è™«å¹¶å¼€å§‹ä¸‹è½½
        spider = RobustImageSpider(folder_name)
        spider.crawl(target_url)

        # è¯¢é—®æ˜¯å¦ç»§ç»­ä¸‹è½½å…¶ä»–é“¾æ¥
        while True:
            try:
                print("\n" + "=" * 60)
                again = input("ğŸ”„ æ˜¯å¦ç»§ç»­ä¸‹è½½å…¶ä»–é“¾æ¥? [y/N]: ").strip().lower()
                if again not in ('y', 'yes', 'æ˜¯'):
                    print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                    break

                # ç»§ç»­ä¸‹è½½æ–°çš„
                new_url = get_user_input()
                if not new_url:
                    break

                new_folder = get_folder_name()
                if not confirm_download(new_url, new_folder):
                    continue

                # åˆ›å»ºæ–°çš„çˆ¬è™«å®ä¾‹ï¼ˆé‡ç½®è®¡æ•°å™¨ï¼‰
                spider = RobustImageSpider(new_folder)
                spider.crawl(new_url)

            except KeyboardInterrupt:
                print("\nğŸ‘‹ ç”¨æˆ·é€€å‡º")
                break

    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå‡ºé”™: {e}")
    finally:
        input("\næŒ‰å›è½¦é”®é€€å‡º...")


if __name__ == "__main__":
    main()

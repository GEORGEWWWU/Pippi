import requests
from bs4 import BeautifulSoup
import re
import os
import time
import random
import hashlib
from urllib.parse import urlparse, unquote
from pathlib import Path


class RobustImageSpider:
    def __init__(self, download_folder="pippi_images"):
        self.download_folder = Path(download_folder)
        self.session = requests.Session()

        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
        ]

        self.session.headers.update(
            {
                "User-Agent": self.user_agents[0],
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9",
            }
        )

        self.download_folder.mkdir(parents=True, exist_ok=True)
        self.downloaded_count = 0
        self.skipped_count = 0
        self.failed_count = 0
        self.image_extensions = (
            ".jpg",
            ".jpeg",
            ".png",
            ".webp",
            ".gif",
            ".bmp",
            ".tiff",
        )
        self.existing_files = self._load_existing_files()

    def _load_existing_files(self):
        existing = set()
        if self.download_folder.exists():
            for f in self.download_folder.iterdir():
                if f.is_file():
                    existing.add(f.stem)
        print(f"ğŸ“‚ å‘ç° {len(existing)} ä¸ªå·²ä¸‹è½½çš„æ–‡ä»¶ï¼Œå°†è‡ªåŠ¨è·³è¿‡")
        return existing

    def _get_random_delay(self, min_sec=1.5, max_sec=3.5):
        return random.uniform(min_sec, max_sec)

    def _is_pixiv_url(self, url):
        """æ£€æŸ¥æ˜¯å¦æ˜¯Pixivç›¸å…³URL"""
        return "pixiv.net" in url.lower() or "pximg.net" in url.lower()

    def _get_headers_for_url(self, url, is_image=False):
        """
        æ ¹æ®URLè·å–å¯¹åº”çš„è¯·æ±‚å¤´
        é’ˆå¯¹Pixivç‰¹æ®Šå¤„ç†ï¼šæ·»åŠ Referer
        """
        headers = {
            "User-Agent": random.choice(self.user_agents),
        }

        if self._is_pixiv_url(url):
            # Pixiv å¿…é¡»æ·»åŠ  Refererï¼Œå¦åˆ™å›¾ç‰‡æœåŠ¡å™¨ä¼šè¿”å› 403
            headers["Referer"] = "https://www.pixiv.net/"
            if is_image:
                headers["Accept"] = "image/webp,image/apng,image/*,*/*;q=0.8"
            else:
                headers["Accept"] = (
                    "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
                )
        else:
            # å…¶ä»–ç½‘ç«™ä½¿ç”¨é€šç”¨å¤´ï¼Œä¸æ·»åŠ Refereré¿å…åçˆ¬
            if is_image:
                headers["Accept"] = "image/webp,image/apng,image/*,*/*;q=0.8"
            else:
                headers["Accept"] = (
                    "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
                )

        return headers

    def get_page(self, url, retries=3):
        for attempt in range(retries):
            try:
                time.sleep(self._get_random_delay(0.5, 1.5))
                headers = self._get_headers_for_url(url, is_image=False)
                r = self.session.get(url, headers=headers, timeout=15)
                r.raise_for_status()
                return r.text
            except Exception as e:
                print(f"  âš ï¸ è·å–å¤±è´¥ (å°è¯• {attempt + 1}/{retries}): {str(e)[:50]}")
                if attempt < retries - 1:
                    time.sleep(2 ** attempt)
        return None

    def extract_images(self, html, base_url=None):
        """
        ä¿®æ”¹ç‰ˆï¼šä¼˜å…ˆä½¿ç”¨ Pixiv Ajax API è·å–é«˜æ¸…åŸå›¾
        """
        if not base_url:
            return []

        images = []

        # === æ ¸å¿ƒä¿®æ”¹ï¼šé’ˆå¯¹ Pixiv ä¼˜å…ˆè°ƒç”¨ API ===
        if self._is_pixiv_url(base_url):
            # 1. å°è¯•ä» URL ä¸­æå– artwork ID
            # åŒ¹é…æ ¼å¼: pixiv.net/artworks/123456 æˆ– pixiv.net/member_illust.php?mode=medium&illust_id=123456
            illust_id = None
            match = re.search(r'artworks/(\d+)', base_url)
            if match:
                illust_id = match.group(1)
            else:
                match = re.search(r'illust_id=(\d+)', base_url)
                if match:
                    illust_id = match.group(1)

            if illust_id:
                print(f" âš™ï¸ æ£€æµ‹åˆ° Pixiv ID: {illust_id}ï¼Œæ­£åœ¨è°ƒç”¨ API...")
                try:
                    # æ„é€  Pixiv å†…éƒ¨ API åœ°å€ (è·å–å¤šå›¾/å•å›¾å‡é€‚ç”¨)
                    api_url = f"https://www.pixiv.net/ajax/illust/{illust_id}/pages?lang=zh"

                    # å¿…é¡»å¸¦ Refererï¼Œå¦åˆ™ API è¿”å› 403
                    headers = self._get_headers_for_url(base_url)

                    # è¯·æ±‚ API
                    api_res = self.session.get(api_url, headers=headers, timeout=10)
                    api_res.raise_for_status()

                    # è§£æ JSON
                    data = api_res.json()

                    if not data.get('error'):
                        # data['body'] æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«æ¯ä¸€é¡µçš„ä¿¡æ¯
                        for page in data.get('body', []):
                            urls = page.get('urls', {})
                            # ä¼˜å…ˆè·å– original (åŸå›¾)ï¼Œå¦‚æœæ²¡æœ‰åˆ™è·å– regular
                            img_url = urls.get('original_pic_url') or urls.get('original') or urls.get('regular')
                            if img_url:
                                images.append(img_url)

                        if images:
                            print(f" Â âœ“ API è°ƒç”¨æˆåŠŸï¼Œè·å–åˆ° {len(images)} å¼ åŸå›¾")
                            return images
                    else:
                        print(f" Â âš ï¸ API è¿”å›é”™è¯¯: {data.get('message')}")

                except Exception as e:
                    print(f" Â âš ï¸ API è°ƒç”¨å¤±è´¥ï¼Œå°è¯•å›é€€åˆ° HTML è§£æ: {e}")

        # === ä»¥ä¸‹æ˜¯å›é€€é€»è¾‘ï¼ˆä½ åŸæœ‰çš„ä»£ç ï¼Œä¿ç•™ä»¥é˜²ä¸‡ä¸€ï¼‰===

        # å°è¯•ä» __NEXT_DATA__ æå– (ä¿ç•™ä½ åŸæœ‰çš„é€»è¾‘ä½œä¸ºå¤‡ä»½)
        import json
        next_data_pattern = r'<script id="__NEXT_DATA__" type="application/json">(.*?)</script>'
        match = re.search(next_data_pattern, html, re.DOTALL)
        if match:
            try:
                data = json.loads(match.group(1))
                # ... (æ­¤å¤„çœç•¥ä½ åŸæœ‰çš„å¤æ‚è§£æé€»è¾‘ï¼Œå¦‚æœä¸Šé¢APIæˆåŠŸï¼Œè¿™é‡Œä¸ä¼šæ‰§è¡Œ) ...
                # ç®€å•å¤„ç†ï¼šå¦‚æœ API å¤±è´¥äº†ï¼Œå°è¯•åœ¨è¿™é‡Œæ‰¾ urls
                if not images:
                    illust_data = data.get('props', {}).get('pageProps', {}).get('illust', {})
                    # ... (ä¸ºäº†ä»£ç ç®€æ´ï¼Œè¿™é‡Œå»ºè®®ç›´æ¥ä¾èµ–ä¸Šé¢çš„ API é€»è¾‘)
            except:
                pass

        # é€šç”¨æ­£åˆ™åŒ¹é…ï¼ˆä½œä¸ºæœ€åçš„å…œåº•ï¼‰
        if not images:
            print(" Â âš ï¸ API å’Œ JSON è§£æå‡å¤±è´¥ï¼Œå°è¯•æš´åŠ›æ­£åˆ™åŒ¹é…...")
            generic_pattern = r'https?://i\.pximg\.net/[^\s"<>\']+?\.(?:jpg|jpeg|png|webp)'
            matches = re.findall(generic_pattern, html, re.IGNORECASE)
            for url in matches:
                # æ¸…æ´— URL
                url = url.strip().rstrip("\"'").replace("\\/", "/")
                # å°è¯•å°†ç¼©ç•¥å›¾è½¬æ¢ä¸ºåŸå›¾
                # ç¼©ç•¥å›¾é€šå¸¸åŒ…å«: _master1200, _square1200, c/600x1200_90 ç­‰
                # åŸå›¾æ ¼å¼é€šå¸¸æ˜¯: https://i.pximg.net/img-original/img/.../xxx_p0.jpg

                # è¿™æ˜¯ä¸€ä¸ªç®€å•çš„æ›¿æ¢å°è¯•ï¼Œä¸ä¸€å®š 100% å‡†ç¡®ï¼Œä½†æ¯”æ²¡æœ‰å¥½
                clean_url = url
                if "_master1200" in url:
                    clean_url = url.replace("_master1200", "")
                    clean_url = clean_url.replace("/img-master/", "/img-original/")
                    # è¿˜éœ€è¦æ³¨æ„åç¼€ï¼Œç¼©ç•¥å›¾å¯èƒ½æ˜¯ jpg ä½†åŸå›¾æ˜¯ png
                    # è¿™é‡Œæ¯”è¾ƒéš¾å¤„ç†ï¼Œæ‰€ä»¥ API æ–¹æ³•æ‰æ˜¯æ­£é“

                if clean_url not in images:
                    images.append(clean_url)

        # å»é‡
        seen = set()
        cleaned = []
        for url in images:
            if url and url not in seen:
                seen.add(url)
                cleaned.append(url)

        return cleaned

    def _is_direct_image_url(self, url):
        """æ£€æŸ¥URLæ˜¯å¦æ˜¯ç›´æ¥çš„å›¾ç‰‡é“¾æ¥"""
        try:
            parsed_url = urlparse(url)
            path = parsed_url.path.lower()

            # æ£€æŸ¥URLè·¯å¾„æ˜¯å¦ä»¥å›¾ç‰‡æ‰©å±•åç»“å°¾
            if any(path.endswith(ext) for ext in self.image_extensions):
                return True

            # æ£€æŸ¥URLè·¯å¾„ä¸­æ˜¯å¦åŒ…å«å›¾ç‰‡æ ¼å¼
            if any(ext in path for ext in self.image_extensions + (".avif",)):
                return True

            return False
        except Exception:
            return False

    def _get_filename(self, url, index):
        try:
            decoded_url = unquote(url)
            parsed = urlparse(decoded_url)
            path = parsed.path
            original_name = Path(path).name

            if original_name and "." in original_name:
                clean_name = re.sub(r'[<>:"/\\|?*]', "_", original_name)
                clean_name = clean_name.split("?")[0].split("@")[0]
                name = Path(clean_name).stem[:50]
                ext = Path(clean_name).suffix.lower()

                # å¤„ç†avifæ ¼å¼ï¼Œè½¬æ¢ä¸ºjpg
                if ext == ".avif":
                    ext = ".jpg"

                if ext not in self.image_extensions:
                    ext = ".jpg"

                return name, ext

        except Exception:
            pass

        # å¦‚æœæ— æ³•ä»URLä¸­æå–æ–‡ä»¶åï¼Œåˆ™ä½¿ç”¨é»˜è®¤å‘½å
        url_hash = hashlib.md5(url.encode()).hexdigest()[:6]
        return f"img_{index:04d}_{url_hash}", ".jpg"

    def _is_exists(self, filename_stem):
        if filename_stem in self.existing_files:
            return True
        for ext in self.image_extensions:
            if (self.download_folder / f"{filename_stem}{ext}").exists():
                return True
        return False

    def download_image(self, url, index, retries=3):
        filename_stem, ext = self._get_filename(url, index)

        if self._is_exists(filename_stem):
            self.skipped_count += 1
            print(f"  â­ï¸  [{index}] {filename_stem}{ext} (å·²å­˜åœ¨)")
            return True

        for attempt in range(retries):
            try:
                delay = min(1.5 + self.downloaded_count * 0.03, 5)
                time.sleep(random.uniform(delay, delay + 1.5))

                # ä½¿ç”¨URLç‰¹å®šçš„è¯·æ±‚å¤´ï¼ˆPixivä¼šæ·»åŠ Refererï¼‰
                headers = self._get_headers_for_url(url, is_image=True)

                # é’ˆå¯¹Pixivçš„ç‰¹æ®Šå¤„ç†ï¼šå¯èƒ½éœ€è¦ç¦ç”¨SSLéªŒè¯
                verify_ssl = True
                if self._is_pixiv_url(url):
                    # Pixivæœ‰æ—¶ä¼šæœ‰SSLè¯ä¹¦é—®é¢˜ï¼Œå¯ä»¥é€‰æ‹©ç¦ç”¨éªŒè¯
                    # æ³¨æ„ï¼šç”Ÿäº§ç¯å¢ƒå»ºè®®ä¿æŒTrueï¼Œé™¤éç¡®å®é‡åˆ°è¯ä¹¦é”™è¯¯
                    pass  # ä¿æŒTrueï¼Œå¦‚æœé‡åˆ°é—®é¢˜å¯ä»¥æ”¹ä¸ºFalse

                r = self.session.get(
                    url, headers=headers, timeout=20, stream=True, verify=verify_ssl
                )
                r.raise_for_status()

                filepath = self.download_folder / f"{filename_stem}{ext}"
                total_size = 0

                with open(filepath, "wb") as f:
                    for chunk in r.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            total_size += len(chunk)

                if total_size < 1024:
                    filepath.unlink()
                    raise ValueError("æ–‡ä»¶è¿‡å°")

                self.existing_files.add(filename_stem)
                self.downloaded_count += 1

                size_kb = total_size / 1024
                print(f"  âœ“ [{index}] {filename_stem}{ext} ({size_kb:.1f} KB)")
                return True

            except Exception as e:
                if attempt < retries - 1:
                    time.sleep(2 ** attempt + random.uniform(0, 1))
                else:
                    self.failed_count += 1
                    print(f"  âŒ [{index}] å¤±è´¥: {str(e)[:40]}")

        return False

    def crawl(self, target_url):
        print(f"\n{'=' * 60}")
        print(f"ğŸš€ çˆ¬å–: {target_url}")
        print(f"ğŸ“ ç›®å½•: {self.download_folder.absolute()}")
        print(f"{'=' * 60}\n")

        # æ£€æŸ¥æ˜¯å¦æ˜¯ç›´æ¥çš„å›¾ç‰‡é“¾æ¥
        if self._is_direct_image_url(target_url):
            print("ğŸ¯ æ£€æµ‹åˆ°ç›´æ¥å›¾ç‰‡é“¾æ¥ï¼Œå¼€å§‹ä¸‹è½½...")
            self.download_image(target_url, 1)
            total = self.downloaded_count + self.skipped_count + self.failed_count
            print(f"\n{'=' * 60}")
            print(
                f"âœ… å®Œæˆ: æ–°ä¸‹è½½ {self.downloaded_count}, è·³è¿‡ {self.skipped_count}, å¤±è´¥ {self.failed_count}"
            )
            print(f"{'=' * 60}")
            return self.downloaded_count

        # åŸæœ‰é€»è¾‘ï¼šä»HTMLé¡µé¢æå–å›¾ç‰‡é“¾æ¥
        html = self.get_page(target_url)
        if not html:
            print("âŒ è·å–é¡µé¢å¤±è´¥")
            return 0

        images = self.extract_images(html, base_url=target_url)
        total = len(images)

        if not images:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡")
            return 0

        print(f"ğŸ¯ å…± {total} å¼ å›¾ç‰‡ï¼Œå¼€å§‹ä¸‹è½½...\n")

        for i, url in enumerate(images, 1):
            self.download_image(url, i)

            if i % 10 == 0 and i < total:
                rest = random.uniform(3, 6)
                print(f"ğŸ’¤ ä¼‘æ¯ {rest:.1f} ç§’...")
                time.sleep(rest)

        print(f"\n{'=' * 60}")
        print(
            f"âœ… å®Œæˆ: æ–°ä¸‹è½½ {self.downloaded_count}, è·³è¿‡ {self.skipped_count}, å¤±è´¥ {self.failed_count}"
        )
        print(f"{'=' * 60}")

        return self.downloaded_count

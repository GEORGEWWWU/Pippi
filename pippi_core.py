import requests
from bs4 import BeautifulSoup
import re
import time
import random
import hashlib
from urllib.parse import urlparse, unquote
from pathlib import Path


class RobustImageSpider:
    def __init__(self, download_folder="pippi_images"):
        self.download_folder = Path(download_folder)
        self.session = requests.Session()

        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
        ]

        # ========== ä¿®æ”¹è¿™é‡Œï¼šæ·»åŠ ä½ çš„ PHPSESSID ==========
        # ä½ åªæœ‰è¿™ä¸€ä¸ªå€¼ï¼Œå…¶ä»–å­—æ®µä¸æ˜¯å¿…é¡»çš„ï¼ŒPHPSESSID æ˜¯æ ¸å¿ƒ
        self.pixiv_cookie = "PHPSESSID=88843137_JNDfSY4N0W1gND6Hu4Iuq3qCO2pFzRh3"
        # ================================================

        self.session.headers.update(
            {
                "User-Agent": self.user_agents[0],
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9",
                # ä¸è¦åœ¨è¿™é‡ŒåŠ  Cookie å’Œ Refererï¼
            }
        )

        self.download_folder.mkdir(parents=True, exist_ok=True)
        self.downloaded_count = 0
        self.skipped_count = 0
        self.failed_count = 0
        self.image_extensions = (
            ".jpg",
            ".jpeg",
            ".png",
            ".webp",
            ".gif",
            ".bmp",
            ".tiff",
        )
        self.existing_files = self._load_existing_files()

    def _load_existing_files(self):
        existing = set()
        if self.download_folder.exists():
            for f in self.download_folder.iterdir():
                if f.is_file():
                    existing.add(f.stem)
        print(f"ğŸ“‚ å‘ç° {len(existing)} ä¸ªå·²ä¸‹è½½çš„æ–‡ä»¶ï¼Œå°†è‡ªåŠ¨è·³è¿‡")
        return existing

    def _get_random_delay(self, min_sec=1.5, max_sec=3.5):
        return random.uniform(min_sec, max_sec)

    def _is_pixiv_url(self, url):
        """æ£€æŸ¥æ˜¯å¦æ˜¯Pixivç›¸å…³URL"""
        return "pixiv.net" in url.lower() or "pximg.net" in url.lower()

    def _is_photos18_url(self, url):
        """æ£€æŸ¥æ˜¯å¦æ˜¯Photos18ç›¸å…³URL"""
        return "photos18.com" in url.lower()

    def _is_foamgirl_url(self, url):
        """æ£€æŸ¥æ˜¯å¦æ˜¯FoamGirlç›¸å…³URL"""
        return "foamgirl.net" in url.lower()

    def _get_headers_for_url(self, url, is_image=False):
        """
        æ ¹æ®URLè·å–å¯¹åº”çš„è¯·æ±‚å¤´
        é’ˆå¯¹Pixivå’ŒPhotos18ç‰¹æ®Šå¤„ç†ï¼šæ·»åŠ Refererå’ŒCookie
        """
        headers = {
            "User-Agent": random.choice(self.user_agents),
        }

        if self._is_pixiv_url(url):
            # Pixiv å¿…é¡»æ·»åŠ  Refererï¼Œå¦åˆ™å›¾ç‰‡æœåŠ¡å™¨ä¼šè¿”å› 403
            headers["Referer"] = "https://www.pixiv.net/"

            # ========== ä¿®æ”¹è¿™é‡Œï¼šæ·»åŠ ä½ çš„ Cookie ==========
            headers["Cookie"] = self.pixiv_cookie
            # ===========================================

            if is_image:
                headers["Accept"] = "image/webp,image/apng,image/*,*/*;q=0.8"
            else:
                headers["Accept"] = (
                    "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
                )
        elif self._is_photos18_url(url):
            # Photos18 ç‰¹æ®Šå¤„ç†
            headers["Referer"] = "https://www.photos18.com/"

            if is_image:
                headers["Accept"] = "image/avif,image/webp,image/apng,image/*,*/*;q=0.8"
            else:
                headers["Accept"] = (
                    "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
                )
        elif self._is_foamgirl_url(url):
            # FoamGirl ç‰¹æ®Šå¤„ç†
            headers["Referer"] = "https://foamgirl.net/"

            if is_image:
                headers["Accept"] = "image/webp,image/apng,image/*,*/*;q=0.8"
            else:
                headers["Accept"] = (
                    "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
                )
        else:
            # å…¶ä»–ç½‘ç«™ä¸åŠ  Referer å’Œ Cookie
            if is_image:
                headers["Accept"] = "image/webp,image/apng,image/*,*/*;q=0.8"
            else:
                headers["Accept"] = (
                    "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
                )

        return headers

    def get_page(self, url, retries=3):
        for attempt in range(retries):
            try:
                time.sleep(self._get_random_delay(0.5, 1.5))
                headers = self._get_headers_for_url(url, is_image=False)
                r = self.session.get(url, headers=headers, timeout=15)
                r.raise_for_status()
                return r.text
            except Exception as e:
                print(f"  âš ï¸ è·å–å¤±è´¥ (å°è¯• {attempt + 1}/{retries}): {str(e)[:50]}")
                if attempt < retries - 1:
                    time.sleep(2**attempt)
        return None

    def extract_images(self, html, base_url=None):
        """
        ä¿®æ”¹ç‰ˆï¼šä¼˜å…ˆä½¿ç”¨ Pixiv Ajax API è·å–é«˜æ¸…åŸå›¾ï¼Œå…¶ä»–ç½‘ç«™ä½¿ç”¨é€šç”¨è§£æ
        """
        if not base_url:
            return []

        images = []

        # === Photos18.com ç‰¹æ®Šå¤„ç† ===
        if "photos18.com" in base_url.lower():
            print(" âš™ï¸ æ£€æµ‹åˆ° Photos18.comï¼Œä½¿ç”¨ç‰¹å®šè§£æè§„åˆ™...")
            soup = BeautifulSoup(html, "html.parser")

            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«å›¾ç‰‡çš„div
            img_holders = soup.find_all("div", class_="imgHolder")
            if not img_holders:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°imgHolderï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„ç±»å
                img_holders = soup.find_all(
                    "div", class_=lambda x: x and "img" in x.lower()
                )

            for holder in img_holders:
                # æŸ¥æ‰¾imgæ ‡ç­¾
                img_tag = holder.find("img")
                if img_tag:
                    src = (
                        img_tag.get("src")
                        or img_tag.get("data-src")
                        or img_tag.get("data-original")
                    )
                    if src and "photos18.com" in src:
                        # å¤„ç†URLï¼Œç¡®ä¿æ˜¯å®Œæ•´URL
                        if src.startswith("//"):
                            src = "https:" + src
                        elif src.startswith("/"):
                            from urllib.parse import urljoin

                            src = urljoin(base_url, src)

                        # ç¡®ä¿æ˜¯avifæ ¼å¼
                        if ".avif" in src:
                            images.append(src)

                # ä¹Ÿæ£€æŸ¥aæ ‡ç­¾çš„href
                a_tag = holder.find("a")
                if a_tag:
                    href = a_tag.get("href")
                    if href and "photos18.com" in href and ".avif" in href:
                        # å¤„ç†URLï¼Œç¡®ä¿æ˜¯å®Œæ•´URL
                        if href.startswith("//"):
                            href = "https:" + href
                        elif href.startswith("/"):
                            from urllib.parse import urljoin

                            href = urljoin(base_url, href)

                        if href not in images:
                            images.append(href)

            # å¦‚æœä¸Šé¢çš„æ–¹æ³•æ²¡æ‰¾åˆ°å›¾ç‰‡ï¼Œå°è¯•æ­£åˆ™åŒ¹é…
            if not images:
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…photos18.comçš„avifå›¾ç‰‡
                pattern = r'https?://(?:www\.)?photos18\.com[^\s<>"{}|\\^`\[\]]*?\.avif[^\s<>"{}|\\^`\[\]]*?'
                matches = re.findall(pattern, html, re.IGNORECASE)
                for url in matches:
                    if url not in images:
                        images.append(url)

            if images:
                print(f"  âœ“ Photos18.com è§£ææ‰¾åˆ° {len(images)} å¼ å›¾ç‰‡")
                # å»é‡åè¿”å›
                seen = set()
                cleaned = []
                for url in images:
                    if url and url not in seen:
                        seen.add(url)
                        cleaned.append(url)
                return cleaned

        # === FoamGirl.net ç‰¹æ®Šå¤„ç† ===
        if "foamgirl.net" in base_url.lower():
            print(" âš™ï¸ æ£€æµ‹åˆ° FoamGirl.netï¼Œä½¿ç”¨ç‰¹å®šè§£æè§„åˆ™...")
            soup = BeautifulSoup(html, "html.parser")

            # æŸ¥æ‰¾æ‰€æœ‰å¸¦æœ‰ imageclick-imgbox ç±»çš„ a æ ‡ç­¾
            img_links = soup.find_all("a", class_="imageclick-imgbox")

            for link in img_links:
                # è·å– href å±æ€§
                href = link.get("href")
                if href and "cdn.foamgirl.net" in href:
                    # å¤„ç†URLï¼Œç¡®ä¿æ˜¯å®Œæ•´URL
                    if href.startswith("//"):
                        href = "https:" + href
                    elif href.startswith("/"):
                        from urllib.parse import urljoin

                        href = urljoin(base_url, href)

                    # ç¡®ä¿æ˜¯webpæ ¼å¼
                    if any(
                        ext in href.lower()
                        for ext in [".webp", ".jpg", ".jpeg", ".png"]
                    ):
                        images.append(href)

                # ä¹Ÿæ£€æŸ¥imgæ ‡ç­¾çš„src
                img_tag = link.find("img")
                if img_tag:
                    src = (
                        img_tag.get("src")
                        or img_tag.get("data-src")
                        or img_tag.get("data-original")
                    )
                    if src and "cdn.foamgirl.net" in src:
                        # å¤„ç†URLï¼Œç¡®ä¿æ˜¯å®Œæ•´URL
                        if src.startswith("//"):
                            src = "https:" + src
                        elif src.startswith("/"):
                            from urllib.parse import urljoin

                            src = urljoin(base_url, src)

                        # ç¡®ä¿æ˜¯webpæ ¼å¼
                        if any(
                            ext in src.lower()
                            for ext in [".webp", ".jpg", ".jpeg", ".png"]
                        ):
                            if src not in images:
                                images.append(src)

            # å¦‚æœä¸Šé¢çš„æ–¹æ³•æ²¡æ‰¾åˆ°å›¾ç‰‡ï¼Œå°è¯•æ­£åˆ™åŒ¹é…
            if not images:
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…foamgirl.netçš„å›¾ç‰‡
                pattern = r'https?://cdn\.foamgirl\.net[^\s<>"{}|\\^`\[\]]*?\.(?:webp|jpg|jpeg|png)[^\s<>"{}|\\^`\[\]]*?'
                matches = re.findall(pattern, html, re.IGNORECASE)
                for url in matches:
                    if url not in images:
                        images.append(url)

            if images:
                print(f"  âœ“ FoamGirl.net è§£ææ‰¾åˆ° {len(images)} å¼ å›¾ç‰‡")
                # å»é‡åè¿”å›
                seen = set()
                cleaned = []
                for url in images:
                    if url and url not in seen:
                        seen.add(url)
                        cleaned.append(url)
                return cleaned
        if self._is_pixiv_url(base_url):
            illust_id = None
            match = re.search(r"artworks/(\d+)", base_url)
            if match:
                illust_id = match.group(1)
            else:
                match = re.search(r"illust_id=(\d+)", base_url)
                if match:
                    illust_id = match.group(1)

            if illust_id:
                print(f" âš™ï¸ æ£€æµ‹åˆ° Pixiv ID: {illust_id}ï¼Œæ­£åœ¨è°ƒç”¨ API...")
                try:
                    # æ³¨æ„ï¼šè¿™é‡Œæœ‰ä¸ªç©ºæ ¼ï¼åˆ é™¤å®ƒ
                    api_url = (
                        f"https://www.pixiv.net/ajax/illust/{illust_id}/pages?lang=zh"
                    )
                    headers = self._get_headers_for_url(base_url)
                    api_res = self.session.get(api_url, headers=headers, timeout=10)
                    api_res.raise_for_status()
                    data = api_res.json()

                    if not data.get("error"):
                        for page in data.get("body", []):
                            urls = page.get("urls", {})
                            img_url = (
                                urls.get("original_pic_url")
                                or urls.get("original")
                                or urls.get("regular")
                            )
                            if img_url:
                                images.append(img_url)

                        if images:
                            print(f"  âœ“ API è°ƒç”¨æˆåŠŸï¼Œè·å–åˆ° {len(images)} å¼ åŸå›¾")
                            return images  # Pixiv æˆåŠŸç›´æ¥è¿”å›
                    else:
                        print(f"  âš ï¸ API è¿”å›é”™è¯¯: {data.get('message')}")

                except Exception as e:
                    print(f"  âš ï¸ API è°ƒç”¨å¤±è´¥ï¼Œå°è¯•å›é€€åˆ° HTML è§£æ: {e}")
                    # Pixiv API å¤±è´¥ç»§ç»­èµ°ä¸‹é¢çš„é€šç”¨è§£æï¼Œä¸è¦ return

        # === é€šç”¨ç½‘ç«™è§£æï¼ˆç™¾åº¦ã€Google ç­‰ï¼‰===
        if not images:  # Pixiv æ²¡æˆåŠŸæˆ–ä¸æ˜¯ Pixivï¼Œæ‰§è¡Œé€šç”¨è§£æ
            print("  ğŸ” ä½¿ç”¨é€šç”¨è§£æè§„åˆ™...")

            # æ–¹æ³•1: ä» img æ ‡ç­¾æå–
            soup = BeautifulSoup(html, "html.parser")
            for img in soup.find_all("img"):
                src = img.get("src") or img.get("data-src") or img.get("data-original")
                if src:
                    # è¡¥å…¨ç›¸å¯¹è·¯å¾„
                    if src.startswith("//"):
                        src = "https:" + src
                    elif src.startswith("/"):
                        from urllib.parse import urljoin

                        src = urljoin(base_url, src)

                    # è¿‡æ»¤å°å›¾æ ‡å’Œæ— æ•ˆé“¾æ¥
                    if any(
                        ext in src.lower()
                        for ext in [".jpg", ".jpeg", ".png", ".webp", ".gif", ".bmp"]
                    ):
                        if not any(
                            x in src
                            for x in ["icon", "logo", "avatar", "thumb", "sprite"]
                        ):
                            images.append(src)

            # æ–¹æ³•2: æ­£åˆ™åŒ¹é… URL æ¨¡å¼çš„å›¾ç‰‡
            if not images:
                url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+?\.(?:jpg|jpeg|png|webp|gif|bmp)(?:\?[^"\s<>]*)?'
                matches = re.findall(url_pattern, html, re.IGNORECASE)
                for url in matches:
                    if url not in images and not any(
                        x in url for x in ["icon", "logo", "avatar"]
                    ):
                        images.append(url)

            if images:
                print(f"  âœ“ é€šç”¨è§£ææ‰¾åˆ° {len(images)} å¼ å›¾ç‰‡")

        # å»é‡
        seen = set()
        cleaned = []
        for url in images:
            if url and url not in seen:
                seen.add(url)
                cleaned.append(url)

        return cleaned

    def _is_direct_image_url(self, url):
        """æ£€æŸ¥URLæ˜¯å¦æ˜¯ç›´æ¥çš„å›¾ç‰‡é“¾æ¥"""
        try:
            parsed_url = urlparse(url)
            path = parsed_url.path.lower()

            # æ£€æŸ¥URLè·¯å¾„æ˜¯å¦ä»¥å›¾ç‰‡æ‰©å±•åç»“å°¾
            if any(path.endswith(ext) for ext in self.image_extensions):
                return True

            # æ£€æŸ¥URLè·¯å¾„ä¸­æ˜¯å¦åŒ…å«å›¾ç‰‡æ ¼å¼
            if any(ext in path for ext in self.image_extensions + (".avif",)):
                return True

            return False
        except Exception:
            return False

    def _get_filename(self, url, index):
        try:
            decoded_url = unquote(url)
            parsed = urlparse(decoded_url)
            path = parsed.path
            original_name = Path(path).name

            if original_name and "." in original_name:
                clean_name = re.sub(r'[<>:"/\\|?*]', "_", original_name)
                clean_name = clean_name.split("?")[0].split("@")[0]
                name = Path(clean_name).stem[:50]
                ext = Path(clean_name).suffix.lower()

                # å¤„ç†avifæ ¼å¼ï¼Œè½¬æ¢ä¸ºjpg
                if ext == ".avif":
                    ext = ".jpg"

                if ext not in self.image_extensions:
                    ext = ".jpg"

                return name, ext

        except Exception:
            pass

        # å¦‚æœæ— æ³•ä»URLä¸­æå–æ–‡ä»¶åï¼Œåˆ™ä½¿ç”¨é»˜è®¤å‘½å
        url_hash = hashlib.md5(url.encode()).hexdigest()[:6]
        return f"img_{index:04d}_{url_hash}", ".jpg"

    def _is_exists(self, filename_stem):
        if filename_stem in self.existing_files:
            return True
        for ext in self.image_extensions:
            if (self.download_folder / f"{filename_stem}{ext}").exists():
                return True
        return False

    def download_image(self, url, index, retries=3):
        filename_stem, ext = self._get_filename(url, index)

        if self._is_exists(filename_stem):
            self.skipped_count += 1
            print(f"  â­ï¸  [{index}] {filename_stem}{ext} (å·²å­˜åœ¨)")
            return True

        for attempt in range(retries):
            try:
                delay = min(1.5 + self.downloaded_count * 0.03, 5)
                time.sleep(random.uniform(delay, delay + 1.5))

                # ä½¿ç”¨URLç‰¹å®šçš„è¯·æ±‚å¤´ï¼ˆPixivä¼šæ·»åŠ Refererï¼‰
                headers = self._get_headers_for_url(url, is_image=True)

                # é’ˆå¯¹Pixivçš„ç‰¹æ®Šå¤„ç†ï¼šå¯èƒ½éœ€è¦ç¦ç”¨SSLéªŒè¯
                verify_ssl = True
                if self._is_pixiv_url(url):
                    # Pixivæœ‰æ—¶ä¼šæœ‰SSLè¯ä¹¦é—®é¢˜ï¼Œå¯ä»¥é€‰æ‹©ç¦ç”¨éªŒè¯
                    # æ³¨æ„ï¼šç”Ÿäº§ç¯å¢ƒå»ºè®®ä¿æŒTrueï¼Œé™¤éç¡®å®é‡åˆ°è¯ä¹¦é”™è¯¯
                    pass  # ä¿æŒTrueï¼Œå¦‚æœé‡åˆ°é—®é¢˜å¯ä»¥æ”¹ä¸ºFalse

                r = self.session.get(
                    url, headers=headers, timeout=20, stream=True, verify=verify_ssl
                )
                r.raise_for_status()

                filepath = self.download_folder / f"{filename_stem}{ext}"
                total_size = 0

                with open(filepath, "wb") as f:
                    for chunk in r.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            total_size += len(chunk)

                if total_size < 1024:
                    filepath.unlink()
                    raise ValueError("æ–‡ä»¶è¿‡å°")

                self.existing_files.add(filename_stem)
                self.downloaded_count += 1

                size_kb = total_size / 1024
                print(f"  âœ“ [{index}] {filename_stem}{ext} ({size_kb:.1f} KB)")
                return True

            except Exception as e:
                if attempt < retries - 1:
                    time.sleep(2**attempt + random.uniform(0, 1))
                else:
                    self.failed_count += 1
                    print(f"  âŒ [{index}] å¤±è´¥: {str(e)[:40]}")

        return False

    def crawl(self, target_url):
        print(f"\n{'=' * 60}")
        print(f"ğŸš€ çˆ¬å–: {target_url}")
        print(f"ğŸ“ ç›®å½•: {self.download_folder.absolute()}")
        print(f"{'=' * 60}\n")

        # æ£€æŸ¥æ˜¯å¦æ˜¯ç›´æ¥çš„å›¾ç‰‡é“¾æ¥
        if self._is_direct_image_url(target_url):
            print("ğŸ¯ æ£€æµ‹åˆ°ç›´æ¥å›¾ç‰‡é“¾æ¥ï¼Œå¼€å§‹ä¸‹è½½...")
            self.download_image(target_url, 1)
            total = self.downloaded_count + self.skipped_count + self.failed_count
            print(f"\n{'=' * 60}")
            print(
                f"âœ… å®Œæˆ: æ–°ä¸‹è½½ {self.downloaded_count}, è·³è¿‡ {self.skipped_count}, å¤±è´¥ {self.failed_count}"
            )
            print(f"{'=' * 60}")
            return self.downloaded_count

        # åŸæœ‰é€»è¾‘ï¼šä»HTMLé¡µé¢æå–å›¾ç‰‡é“¾æ¥
        html = self.get_page(target_url)
        if not html:
            print("âŒ è·å–é¡µé¢å¤±è´¥")
            return 0

        images = self.extract_images(html, base_url=target_url)
        total = len(images)

        if not images:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡")
            return 0

        print(f"ğŸ¯ å…± {total} å¼ å›¾ç‰‡ï¼Œå¼€å§‹ä¸‹è½½...\n")

        for i, url in enumerate(images, 1):
            self.download_image(url, i)

            if i % 10 == 0 and i < total:
                rest = random.uniform(3, 6)
                print(f"ğŸ’¤ ä¼‘æ¯ {rest:.1f} ç§’...")
                time.sleep(rest)

        print(f"\n{'=' * 60}")
        print(
            f"âœ… å®Œæˆ: æ–°ä¸‹è½½ {self.downloaded_count}, è·³è¿‡ {self.skipped_count}, å¤±è´¥ {self.failed_count}"
        )
        print(f"{'=' * 60}")

        return self.downloaded_count
